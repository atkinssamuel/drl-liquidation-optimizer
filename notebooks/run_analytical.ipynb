{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Importing\n",
    "from ppo.environments.ma_almgrenchriss import MultiAgentAlmgrenChriss\n",
    "from ppo.src.evaluate import evaluate_agents\n",
    "from shared.constants import PPOAgent1Params, PPOAgent2Params, PPOTrainingParams, PPOAgent3Params, PPOEnvParams\n",
    "\n",
    "from ppo.src.agent import PPOAgent\n",
    "from ppo.src.train import train_multi_agent_ppo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0/100:     Reward for Agent 1 = -23,215,754,582.85    Reward for Agent 2 = -18,422,750,000.00    \n",
      "Episode 5/100:     Reward for Agent 1 = -25,988,024,115.81    Reward for Agent 2 = -18,422,750,000.00    \n",
      "Episode 10/100:    Reward for Agent 1 = -18,677,078,888.00    Reward for Agent 2 = -22,372,533,189.10    \n",
      "Episode 15/100:    Reward for Agent 1 = -32,056,169,309.06    Reward for Agent 2 = -18,422,750,000.00    \n",
      "Episode 20/100:    Reward for Agent 1 = -28,634,478,025.33    Reward for Agent 2 = -24,980,973,606.91    \n",
      "Episode 25/100:    Reward for Agent 1 = -34,566,489,090.90    Reward for Agent 2 = -54,650,525,866.87    \n",
      "Episode 30/100:    Reward for Agent 1 = -54,265,742,010.30    Reward for Agent 2 = -71,436,440,768.64    \n",
      "Episode 35/100:    Reward for Agent 1 = -25,900,542,747.84    Reward for Agent 2 = -39,957,784,371.32    \n",
      "Episode 40/100:    Reward for Agent 1 = -19,453,136,685.46    Reward for Agent 2 = -29,970,338,190.47    \n",
      "Episode 45/100:    Reward for Agent 1 = -21,183,974,691.54    Reward for Agent 2 = -27,912,911,045.65    \n",
      "Episode 50/100:    Reward for Agent 1 = -22,739,983,826.11    Reward for Agent 2 = -31,000,670,556.55    \n",
      "Episode 55/100:    Reward for Agent 1 = -22,945,356,996.47    Reward for Agent 2 = -35,292,177,645.05    \n",
      "Episode 60/100:    Reward for Agent 1 = -20,483,932,130.66    Reward for Agent 2 = -38,120,710,129.00    \n",
      "Episode 65/100:    Reward for Agent 1 = -22,230,552,927.94    Reward for Agent 2 = -35,232,467,129.73    \n",
      "Episode 70/100:    Reward for Agent 1 = -27,866,962,438.51    Reward for Agent 2 = -32,942,533,216.73    \n",
      "Episode 75/100:    Reward for Agent 1 = -28,778,637,247.88    Reward for Agent 2 = -33,408,849,242.90    \n",
      "Episode 80/100:    Reward for Agent 1 = -24,443,772,777.29    Reward for Agent 2 = -27,926,162,555.82    \n",
      "Episode 85/100:    Reward for Agent 1 = -23,488,530,287.01    Reward for Agent 2 = -32,630,422,790.38    \n",
      "Episode 90/100:    Reward for Agent 1 = -19,662,206,823.09    Reward for Agent 2 = -31,116,163,833.83    \n",
      "Episode 95/100:    Reward for Agent 1 = -23,093,185,589.46    Reward for Agent 2 = -28,659,063,666.87    \n"
     ]
    }
   ],
   "source": [
    "agent_1_args = PPOAgent1Params()\n",
    "agent_2_args = PPOAgent2Params()\n",
    "agent_3_args = PPOAgent3Params()\n",
    "ppo_training_params = PPOTrainingParams()\n",
    "env_params = PPOEnvParams()\n",
    "\n",
    "env = MultiAgentAlmgrenChriss(agent_1_args, agent_2_args, env_params=env_params)\n",
    "\n",
    "agent_1 = PPOAgent(agent_1_args, env=env)\n",
    "agent_2 = PPOAgent(agent_2_args, env=env)\n",
    "agent_3 = PPOAgent(agent_3_args, env=env)\n",
    "\n",
    "train_multi_agent_ppo(agent_1, agent_2, env=env, ppo_training_params=ppo_training_params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'bool' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-9-727d90f3232c>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mevaluate_agents\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0magent_1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0menv\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0menv\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\projects\\drl-liquidation-optimizer\\ppo\\src\\evaluate.py\u001B[0m in \u001B[0;36mevaluate_agents\u001B[1;34m(env, *agents)\u001B[0m\n\u001B[0;32m     33\u001B[0m                 \u001B[0maction_dict\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'values'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     34\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 35\u001B[1;33m                 \u001B[0mstep_dict\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0menv\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0maction_dict\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     36\u001B[0m                 \u001B[0mdone\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstep_dict\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'dones'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     37\u001B[0m                 \u001B[0mobservation_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstep_dict\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'state'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\projects\\drl-liquidation-optimizer\\ppo\\environments\\ma_almgrenchriss.py\u001B[0m in \u001B[0;36mstep\u001B[1;34m(self, multi_agent_action_dict)\u001B[0m\n\u001B[0;32m    115\u001B[0m             \u001B[0magent\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmulti_agent_action_dict\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'agents'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    116\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 117\u001B[1;33m             \u001B[1;32mif\u001B[0m \u001B[0mdones\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    118\u001B[0m                 \u001B[0mtotal_permanent_price_impact\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[1;33m-\u001B[0m\u001B[0magent\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    119\u001B[0m                 \u001B[1;32mcontinue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: 'bool' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "evaluate_agents(agent_1, env=env)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}